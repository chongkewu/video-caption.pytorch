{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from models import EncoderRNN, DecoderRNN, S2VTAttModel, S2VTModel\n",
    "from dataloader import VideoDataset\n",
    "import misc.utils as utils\n",
    "from misc.cocoeval import suppress_stdout_stderr, COCOScorer\n",
    "import numpy as np\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {'recover_opt': 'data/save/opt_info.json', 'saved_model': 'data/save/model_50.pth', 'dump_json': 1, 'results_path': 'results/', 'dump_path': 0, 'gpu': '0', 'batch_size': 25, 'sample_max': 1, 'temperature': 1.0, 'beam_size': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = json.load(open(args[\"recover_opt\"]))\n",
    "for k, v in args.items():\n",
    "        opt[k] = v\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = opt[\"gpu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is  16860\n",
      "number of train videos:  6501\n",
      "number of val videos:  500\n",
      "number of test videos:  2999\n",
      "load feats from ['data/feats/resnet152']\n",
      "max sequence length in data is 28\n"
     ]
    }
   ],
   "source": [
    "dataset = VideoDataset(opt, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt[\"vocab_size\"] = dataset.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt[\"seq_length\"] = dataset.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S2VTAttModel'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chongke/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(opt[\"dim_vid\"], opt[\"dim_hidden\"], bidirectional=opt[\"bidirectional\"],\n",
    "                             input_dropout_p=opt[\"input_dropout_p\"], rnn_dropout_p=opt[\"rnn_dropout_p\"]);\n",
    "decoder = DecoderRNN(opt[\"vocab_size\"], opt[\"max_len\"], opt[\"dim_hidden\"], opt[\"dim_word\"],\n",
    "                             input_dropout_p=opt[\"input_dropout_p\"],\n",
    "                             rnn_dropout_p=opt[\"rnn_dropout_p\"], bidirectional=opt[\"bidirectional\"]);\n",
    "model = S2VTAttModel(encoder, decoder).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(opt[\"saved_model\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chongke/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "crit = utils.LanguageModelCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.encoder.rnn.bidirectional = bool(model.encoder.rnn.bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = dataset.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2VTAttModel(\n",
       "  (encoder): EncoderRNN(\n",
       "    (vid2hid): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (input_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (rnn): GRU(512, 512, batch_first=True, dropout=0.5)\n",
       "  )\n",
       "  (decoder): DecoderRNN(\n",
       "    (input_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (embedding): Embedding(16860, 512)\n",
       "    (attention): Attention(\n",
       "      (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (rnn): GRU(1024, 512, batch_first=True, dropout=0.5)\n",
       "    (out): Linear(in_features=512, out_features=16860, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loader = DataLoader(dataset, batch_size=opt[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init COCO-EVAL scorer\n"
     ]
    }
   ],
   "source": [
    "scorer = COCOScorer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_dataframe = json_normalize(\n",
    "        json.load(open(opt[\"input_json\"]))['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_data_to_coco_scorer_format(data_frame):\n",
    "    gts = {}\n",
    "    for row in zip(data_frame[\"caption\"], data_frame[\"video_id\"]):\n",
    "        if row[1] in gts:\n",
    "            gts[row[1]].append(\n",
    "                {'image_id': row[1], 'cap_id': len(gts[row[1]]), 'caption': row[0]})\n",
    "        else:\n",
    "            gts[row[1]] = []\n",
    "            gts[row[1]].append(\n",
    "                {'image_id': row[1], 'cap_id': len(gts[row[1]]), 'caption': row[0]})\n",
    "    return gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gts = convert_data_to_coco_scorer_format(gt_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_label = [' '.join([e for e in [vocab.get(str(key.data.tolist())) for key in data['labels'][ind]] if e not in ('<eos>', '<sos>')]) for ind in range(data['labels'].shape[0])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/feats/resnet152']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt[\"feats_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 1\n",
    "fc_feat = []\n",
    "fc_feat.append(np.load(os.path.join(opt[\"feats_dir\"][0], 'video%i.npy' % (ix))))\n",
    "fc_feat = np.concatenate(fc_feat, axis=1)\n",
    "fc_feat = fc_feat[None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_feat=torch.from_numpy(fc_feat).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 2048])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_feat = fc_feat.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chongke/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1339: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "# forward the model to also get generated samples for each image\n",
    "with torch.no_grad():\n",
    "    seq_probs, seq_preds = model(\n",
    "        fc_feat, mode='inference', opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = utils.decode_sequence(vocab, seq_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_result = [video_ids, sents, sents_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 150\n",
    "df_result = pd.DataFrame(sample_result).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_result.columns = ['id', 'inference', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>inference</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video7114</td>\n",
       "      <td>a man is cooking</td>\n",
       "      <td>a man and a woman talking to each other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id         inference                                    label\n",
       "0  video7114  a man is cooking  a man and a woman talking to each other"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>sen_id</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59900</th>\n",
       "      <td>a girl and a man are talking to each other</td>\n",
       "      <td>59900</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59901</th>\n",
       "      <td>a girl helps a man detective prevent crimes</td>\n",
       "      <td>59901</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59902</th>\n",
       "      <td>a man and a woman talking to each other</td>\n",
       "      <td>59902</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59903</th>\n",
       "      <td>a man and woman are seriously discussing about something</td>\n",
       "      <td>59903</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59904</th>\n",
       "      <td>a man speaks to a woman in a doorway</td>\n",
       "      <td>59904</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59905</th>\n",
       "      <td>a man stands outside a glass door</td>\n",
       "      <td>59905</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59906</th>\n",
       "      <td>a movie trailer about a man investigating a murder</td>\n",
       "      <td>59906</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59907</th>\n",
       "      <td>a narrator talks about a show s plot</td>\n",
       "      <td>59907</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59908</th>\n",
       "      <td>a show is voiced over</td>\n",
       "      <td>59908</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59909</th>\n",
       "      <td>a thriller movie trailer</td>\n",
       "      <td>59909</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59910</th>\n",
       "      <td>a woman talking to a man</td>\n",
       "      <td>59910</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59911</th>\n",
       "      <td>an intro to a television show</td>\n",
       "      <td>59911</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59912</th>\n",
       "      <td>girl and boy discuss murder mystery</td>\n",
       "      <td>59912</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59913</th>\n",
       "      <td>man and woman in different parts of a building</td>\n",
       "      <td>59913</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59914</th>\n",
       "      <td>people talking about a murderer</td>\n",
       "      <td>59914</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59915</th>\n",
       "      <td>several people arguing in a mystery story</td>\n",
       "      <td>59915</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59916</th>\n",
       "      <td>some people are talking</td>\n",
       "      <td>59916</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59917</th>\n",
       "      <td>a man stands outside a glass door</td>\n",
       "      <td>59917</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59918</th>\n",
       "      <td>people talking about a murderer</td>\n",
       "      <td>59918</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59919</th>\n",
       "      <td>a woman talking to a man</td>\n",
       "      <td>59919</td>\n",
       "      <td>video7114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        caption  sen_id  \\\n",
       "59900                a girl and a man are talking to each other   59900   \n",
       "59901               a girl helps a man detective prevent crimes   59901   \n",
       "59902                   a man and a woman talking to each other   59902   \n",
       "59903  a man and woman are seriously discussing about something   59903   \n",
       "59904                      a man speaks to a woman in a doorway   59904   \n",
       "59905                         a man stands outside a glass door   59905   \n",
       "59906        a movie trailer about a man investigating a murder   59906   \n",
       "59907                      a narrator talks about a show s plot   59907   \n",
       "59908                                     a show is voiced over   59908   \n",
       "59909                                  a thriller movie trailer   59909   \n",
       "59910                                  a woman talking to a man   59910   \n",
       "59911                             an intro to a television show   59911   \n",
       "59912                       girl and boy discuss murder mystery   59912   \n",
       "59913            man and woman in different parts of a building   59913   \n",
       "59914                           people talking about a murderer   59914   \n",
       "59915                 several people arguing in a mystery story   59915   \n",
       "59916                                   some people are talking   59916   \n",
       "59917                         a man stands outside a glass door   59917   \n",
       "59918                           people talking about a murderer   59918   \n",
       "59919                                  a woman talking to a man   59919   \n",
       "\n",
       "        video_id  \n",
       "59900  video7114  \n",
       "59901  video7114  \n",
       "59902  video7114  \n",
       "59903  video7114  \n",
       "59904  video7114  \n",
       "59905  video7114  \n",
       "59906  video7114  \n",
       "59907  video7114  \n",
       "59908  video7114  \n",
       "59909  video7114  \n",
       "59910  video7114  \n",
       "59911  video7114  \n",
       "59912  video7114  \n",
       "59913  video7114  \n",
       "59914  video7114  \n",
       "59915  video7114  \n",
       "59916  video7114  \n",
       "59917  video7114  \n",
       "59918  video7114  \n",
       "59919  video7114  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_dataframe.loc[gt_dataframe.index[gt_dataframe['video_id'] == video_ids[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
